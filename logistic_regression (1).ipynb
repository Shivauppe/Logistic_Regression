{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10083b6",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60eb763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17480522",
   "metadata": {},
   "source": [
    "### 1. What is Logistic Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26b20b",
   "metadata": {},
   "source": [
    "Logistic Regression is a classification algorithm used to predict binary outcomes. It models the probability that a given input belongs to a specific category using the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940847b",
   "metadata": {},
   "source": [
    "### 2. Mathematical Equation of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbc302",
   "metadata": {},
   "source": [
    "$h_{\\theta}(x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83392728",
   "metadata": {},
   "source": [
    "### 3. Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c3860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c2efc",
   "metadata": {},
   "source": [
    "### 4. Cost Function of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa24f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_loss(y_true, y_pred):\n",
    "    m = len(y_true)\n",
    "    return - (1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9010851",
   "metadata": {},
   "source": [
    "### 5. Regularization in Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b81dc",
   "metadata": {},
   "source": [
    "Regularization prevents overfitting by penalizing large coefficients. It includes L1 (Lasso), L2 (Ridge), and Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354a2dc",
   "metadata": {},
   "source": [
    "### 6. Difference between Lasso, Ridge, and Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb4c73",
   "metadata": {},
   "source": [
    "- **Lasso (L1):** Shrinks some coefficients to 0 (feature selection).  \n",
    "- **Ridge (L2):** Shrinks coefficients but retains all features.  \n",
    "- **Elastic Net:** Combines both L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1aceee",
   "metadata": {},
   "source": [
    "### 7. Choosing Elastic Net over Lasso or Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d62207",
   "metadata": {},
   "source": [
    "Use Elastic Net when features are highly correlated or when Lasso is too aggressive in feature elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7240b",
   "metadata": {},
   "source": [
    "### 8. Impact of Regularization Parameter (λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c2ba4",
   "metadata": {},
   "source": [
    "Higher λ: More regularization → Simpler model, avoids overfitting.  \n",
    "Lower λ: Less regularization → More complex model, risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776abe1c",
   "metadata": {},
   "source": [
    "### 9. Assumptions of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e09df9",
   "metadata": {},
   "source": [
    "1. Observations are independent.\n",
    "2. No multicollinearity.\n",
    "3. Linear relationship between features and log-odds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cacc3b",
   "metadata": {},
   "source": [
    "### 10. Alternatives to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c240c",
   "metadata": {},
   "source": [
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Support Vector Machines (SVM)\n",
    "- Naïve Bayes\n",
    "- Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d5281",
   "metadata": {},
   "source": [
    "### 11. Classification Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc3c3d5",
   "metadata": {},
   "source": [
    "- Accuracy\n",
    "- Precision, Recall, F1-score\n",
    "- ROC-AUC\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f7be1",
   "metadata": {},
   "source": [
    "### 12. Class Imbalance in Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e2c1b4",
   "metadata": {},
   "source": [
    "- Causes biased predictions towards the majority class.  \n",
    "- **Solutions:** Class weighting, oversampling/undersampling, using F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eed200",
   "metadata": {},
   "source": [
    "### 13. Hyperparameter Tuning in Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f6f716",
   "metadata": {},
   "source": [
    "Hyperparameters like regularization strength (λ) can be tuned using GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce26af4",
   "metadata": {},
   "source": [
    "### 14. Solvers in Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "solvers = ['liblinear', 'lbfgs', 'newton-cg', 'saga']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48526c3f",
   "metadata": {},
   "source": [
    "- **'liblinear'** → Small datasets, supports L1 & L2.  \n",
    "- **'lbfgs'** → Default solver, works for multiclass.  \n",
    "- **'newton-cg'** → Good for L2.  \n",
    "- **'saga'** → Suitable for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa30572",
   "metadata": {},
   "source": [
    "### 15. Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d9fd0",
   "metadata": {},
   "source": [
    "**One-vs-Rest (OvR):** Trains multiple binary classifiers.  \n",
    "**Softmax Regression:** Assigns probabilities to multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c358a",
   "metadata": {},
   "source": [
    "### 16. Advantages & Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2f9d5",
   "metadata": {},
   "source": [
    "**Advantages:**\n",
    "- Simple, interpretable, efficient.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Assumes linearity in log-odds.\n",
    "- Sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c54473",
   "metadata": {},
   "source": [
    "### 17. Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e2d85",
   "metadata": {},
   "source": [
    "- Medical Diagnosis\n",
    "- Credit Scoring\n",
    "- Spam Detection\n",
    "- Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431db88",
   "metadata": {},
   "source": [
    "### 18. Softmax Regression vs Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca1dda7",
   "metadata": {},
   "source": [
    "- **Logistic Regression** → Binary Classification.\n",
    "- **Softmax Regression** → Multiclass Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128ddeb7",
   "metadata": {},
   "source": [
    "### 19. Choosing OvR vs Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b38563",
   "metadata": {},
   "source": [
    "- **OvR:** Small datasets, interpretable.\n",
    "- **Softmax:** Large datasets, better probabilistic interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5804078",
   "metadata": {},
   "source": [
    "### 20. Interpreting Coefficients in Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def interpret_coefficients(coefs):\n",
    "    return np.exp(coefs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c83977",
   "metadata": {},
   "source": [
    "Exp(theta) gives the odds ratio → A value >1 means increased odds, <1 means decreased odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c69e06-ed9d-4368-8bbd-5b66d25aedff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy\n",
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris  # Example dataset\n",
    "\n",
    "# Load a sample dataset (Iris dataset)\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6dfb1d-268e-4ec2-b434-b662387d9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
    "\n",
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris  # Example dataset\n",
    "\n",
    "# Load a sample dataset (Iris dataset)\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model with L1 regularization (Lasso)\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Model Accuracy with L1 Regularization (Lasso): {accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b329d51-8d34-4014-9d2f-4d9f7a679892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
    "\n",
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris  # Example dataset\n",
    "\n",
    "# Load a sample dataset (Iris dataset)\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model with L2 regularization (Ridge)\n",
    "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy and model coefficients\n",
    "print(f\"Model Accuracy with L2 Regularization (Ridge): {accuracy:.4f}\")\n",
    "print(\"Model Coefficients:\")\n",
    "print(model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2aca8-6169-4fa2-a0cf-6d80614aac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4aaa5d-57b5-4852-9979-a4d19e7d6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore all warnings\n",
    "def train_logistic_regression_elasticnet():\n",
    "    # Load sample dataset (Iris dataset)\n",
    "    data = load_iris()\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    y = pd.Series(data.target)\n",
    "    \n",
    "    # Initialize the Logistic Regression model with Elastic Net regularization\n",
    "    model = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=200, l1_ratio=0.5)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions and calculate accuracy\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    \n",
    "    print(f\"Elastic Net Regularization Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "train_logistic_regression_elasticnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc003f8-b29b-4e05-979b-b1de03f1a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.Logistic Regression for Multiclass Classification using 'ovr' (One-vs-Rest)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore all warnings\n",
    "def train_logistic_regression_multiclass():\n",
    "    # Load sample dataset (Iris dataset)\n",
    "    data = load_iris()\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    y = pd.Series(data.target)\n",
    "    \n",
    "    # Initialize the Logistic Regression model with 'ovr' (One-vs-Rest)\n",
    "    model = LogisticRegression(multi_class='ovr', max_iter=200)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions and calculate accuracy\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    \n",
    "    print(f\"Multiclass Logistic Regression (OVR) Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "train_logistic_regression_multiclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff8797-d0a1-4b6d-b5fa-bfa0a7f8f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.GridSearchCV to tune hyperparameters (C and penalty)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore all warnings\n",
    "\n",
    "def tune_logistic_regression_hyperparameters():\n",
    "    # Load sample dataset (Iris dataset)\n",
    "    data = load_iris()\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    y = pd.Series(data.target)\n",
    "    \n",
    "    # Define Logistic Regression model\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    \n",
    "    # Define hyperparameters for GridSearchCV\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    }\n",
    "    \n",
    "    # Apply GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Get best parameters and accuracy\n",
    "    print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "tune_logistic_regression_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f47a47-7458-4751-b999-bbec20d8f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.Logistic Regression with Stratified K-Fold Cross-Validation\n",
    "def evaluate_logistic_regression_stratified_kfold():\n",
    "    # Load sample dataset (Iris dataset)\n",
    "    data = load_iris()\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    y = pd.Series(data.target)\n",
    "    \n",
    "    # Initialize Logistic Regression model\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    \n",
    "    # Perform Stratified K-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cross_val_scores = cross_val_score(model, X, y, cv=skf)\n",
    "    \n",
    "    # Print average accuracy\n",
    "    print(f\"Average Accuracy using Stratified K-Fold CV: {cross_val_scores.mean():.4f}\")\n",
    "\n",
    "evaluate_logistic_regression_stratified_kfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5275c8e-829d-478e-a825-e6c421198e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "def load_and_evaluate_logistic_regression(csv_file):\n",
    "    # Load dataset from CSV file\n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Display the first few rows of the dataset to understand its structure\n",
    "    print(\"Dataset Preview:\")\n",
    "    print(data.head())\n",
    "    \n",
    "    # Handle missing values\n",
    "    # For features, you can fill missing values with the column mean or drop rows with missing values.\n",
    "    data = data.dropna()  # Drop rows with any NaN values\n",
    "\n",
    "    # Separate features (X) and target variable (y)\n",
    "    X = data.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "    y = data.iloc[:, -1]   # Target (the last column)\n",
    "    \n",
    "    # Handle non-numeric columns in the features by encoding them\n",
    "    for column in X.select_dtypes(include=['object']).columns:\n",
    "        label_encoder = LabelEncoder()\n",
    "        X[column] = label_encoder.fit_transform(X[column])\n",
    "    \n",
    "    # Handle non-numeric target variable (if applicable)\n",
    "    if y.dtype == 'object':\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Preprocess: Scaling features (important for regularization methods)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize Logistic Regression model\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print the accuracy\n",
    "    print(f\"Logistic Regression Model Accuracy on Loaded CSV Data: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "load_and_evaluate_logistic_regression('/Users/sangameshwaruppe/custom_app/BIKE DETAILS.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8fb11b-baa6-4294-b7c9-85f885002f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Hyperparameters for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(log_reg, param_distributions=param_dist, n_iter=100, random_state=42, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7f86c-a03c-4464-8e07-04e69f025bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize logistic regression and OvO classifier\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "ovo_classifier = OneVsOneClassifier(log_reg)\n",
    "\n",
    "# Train the OvO classifier\n",
    "ovo_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = ovo_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of One-vs-One Multiclass Logistic Regression: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ef555-07be-4f5c-996b-b001986ad4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a032b-acb9-4fb5-a364-896cebf4108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate Precision, Recall, F1-Score, and Accuracy\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af4048a-9790-49d8-a0aa-978e7f5488c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Generate an imbalanced binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.8, 0.2], random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Logistic Regression model with class weights\n",
    "log_reg = LogisticRegression(class_weight='balanced')  # Automatically adjust weights based on the class distribution\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate and print the evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38c991-3d79-4baa-85bf-0dfaaaf0556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Titanic dataset (ensure to have 'train.csv' file in the working directory or provide the URL)\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Handle missing values:\n",
    "# Use SimpleImputer to fill missing numerical values with the median and categorical with the most frequent value\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "#data['Embarked'] = imputer.fit_transform(data[['Embarked']])\n",
    "\n",
    "# For numerical values, use median for missing 'Age' and 'Fare'\n",
    "data['Age'] = SimpleImputer(strategy='median').fit_transform(data[['Age']])\n",
    "data['Fare'] = SimpleImputer(strategy='median').fit_transform(data[['Fare']])\n",
    "\n",
    "# Drop the 'Cabin' column as it's too sparse and may not be useful\n",
    "data = data.drop(columns=['Cabin'])\n",
    "\n",
    "# Drop 'Name' and 'Ticket' columns since they don't provide numerical value for prediction\n",
    "data = data.drop(columns=['Name', 'Ticket'])\n",
    "\n",
    "# Convert categorical variables 'Sex' and 'Embarked' into numerical values using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sex'] = label_encoder.fit_transform(data['Sex'])\n",
    "data['Embarked'] = label_encoder.fit_transform(data['Embarked'])\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = data.drop(columns=['Survived'])\n",
    "y = data['Survived']\n",
    "\n",
    "# Split the dataset into training and testing sets (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the feature values (optional but often useful for Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print classification report for detailed performance metrics\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2fcd35-ddb3-48a1-8459-f528e15fa8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Logistic Regression on raw (unscaled) data\n",
    "model_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_raw.fit(X_train, y_train)\n",
    "y_pred_raw = model_raw.predict(X_test)\n",
    "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
    "\n",
    "# Standardize the data (feature scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression on standardized data\n",
    "model_scaled = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# Display results\n",
    "print(f\"Accuracy with raw (unscaled) data: {accuracy_raw:.4f}\")\n",
    "print(f\"Accuracy with standardized data: {accuracy_scaled:.4f}\")\n",
    "\n",
    "# Plot the accuracies for comparison\n",
    "labels = ['Raw Data', 'Standardized Data']\n",
    "accuracies = [accuracy_raw, accuracy_scaled]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, accuracies, color=['blue', 'green'])\n",
    "plt.xlabel('Data Type')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparison of Logistic Regression Accuracy with and without Feature Scaling')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4f120-d158-40e0-a2c5-8f30263e791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset (e.g., breast cancer dataset for binary classification)\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model using ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f'ROC-AUC Score: {roc_auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f56062-bc36-48af-91bc-2e5aa4261eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset (e.g., breast cancer dataset for binary classification)\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Logistic Regression model with a custom learning rate (C=0.5)\n",
    "model = LogisticRegression(C=0.5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac8c6a-2cf7-45a4-9176-2fe3cf553d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset (e.g., breast cancer dataset for binary classification)\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Get the model coefficients and associate them with feature names\n",
    "coefficients = model.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients,\n",
    "    'Importance': np.abs(coefficients)  # Absolute value of coefficients to measure importance\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the top 10 important features\n",
    "print(\"\\nTop 10 Important Features Based on Coefficients:\")\n",
    "print(feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7320ca3-0b04-4b3c-ae71-37934c3f29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset (e.g., breast cancer dataset for binary classification)\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance using Cohen's Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(f'Cohen’s Kappa Score: {kappa_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c6166-b075-4366-9e84-968c63f0d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get the predicted probabilities for the positive class\n",
    "y_scores = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "# Compute the AUC of the Precision-Recall curve\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot the Precision-Recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='b', label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da426c0-e2d4-431d-b03e-0ce79a77b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21.  Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define solvers\n",
    "solvers = ['liblinear', 'saga', 'lbfgs']\n",
    "accuracies = []\n",
    "\n",
    "# Train Logistic Regression with different solvers and compute accuracy\n",
    "for solver in solvers:\n",
    "    model = LogisticRegression(solver=solver, max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Display results\n",
    "for solver, accuracy in zip(solvers, accuracies):\n",
    "    print(f\"Accuracy with solver '{solver}': {accuracy:.4f}\")\n",
    "\n",
    "# Plot the accuracies of the different solvers\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(solvers, accuracies, color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Solver')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparison of Logistic Regression Accuracy with Different Solvers')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebcba2e-d056-4708-9dbf-88f2e9d87e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient (MCC)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90106db0-3bd3-4618-9d89-a5170d408a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Logistic Regression on raw data\n",
    "model_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_raw.fit(X_train, y_train)\n",
    "y_pred_raw = model_raw.predict(X_test)\n",
    "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression on standardized data\n",
    "model_scaled = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# Display results\n",
    "print(f\"Accuracy with raw data: {accuracy_raw:.4f}\")\n",
    "print(f\"Accuracy with standardized data: {accuracy_scaled:.4f}\")\n",
    "\n",
    "# Plot the accuracies\n",
    "labels = ['Raw Data', 'Standardized Data']\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178de890-0ba5-4222-8a48-fb77cab0f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Define the range of C values to search over (inverse regularization strength)\n",
    "param_grid = {'C': np.logspace(-4, 4, 20)}  # Search C from 10^-4 to 10^4\n",
    "\n",
    "# Use GridSearchCV to find the optimal C using cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best C value and the corresponding accuracy\n",
    "optimal_C = grid_search.best_params_['C']\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "# Display the results\n",
    "print(f\"Optimal C: {optimal_C}\")\n",
    "print(f\"Best cross-validation accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model with the optimal C on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Display test accuracy\n",
    "print(f\"Test accuracy with optimal C: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot the cross-validation results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.semilogx(grid_search.cv_results_['param_C'], grid_search.cv_results_['mean_test_score'], marker='o')\n",
    "plt.xlabel('Regularization strength (C)')\n",
    "plt.ylabel('Mean cross-validation accuracy')\n",
    "plt.title('Grid Search - Cross-validation Accuracy vs C')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab542a-7cce-4b01-943f-440723111fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model using joblib\n",
    "joblib_file = \"logistic_regression_model.joblib\"\n",
    "joblib.dump(model, joblib_file)\n",
    "print(f\"Model saved to {joblib_file}\")\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = joblib.load(joblib_file)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the loaded model: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803315d2-b312-41c2-8892-aa44085d5e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
